import streamlit as st
import os
import re
import PyPDF2
from nltk.tokenize import sent_tokenize
import nltk
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer


# NLTK'nÄ±n gerekli veri setlerini indirme (ilk Ã§alÄ±ÅŸtÄ±rmada gerekli)
nltk_resources = ['punkt', 'punkt_tab']

for resource in nltk_resources:
    try:
        nltk.data.find(f'tokenizers/{resource}')
    except LookupError:
        nltk.download(resource)

# Sentence-transformers modelini yÃ¼kleme (hafif bir model seÃ§ildi)
@st.cache_resource
def load_model():
    model = SentenceTransformer('paraphrase-MiniLM-L3-v2')  # Hafif bir model
    return model

def extract_text_from_pdf(pdf_file):
    """PDF dosyasÄ±ndan tam metni Ã§Ä±karÄ±r."""
    text = ""
    pdf_reader = PyPDF2.PdfReader(pdf_file)
    for page_num in range(len(pdf_reader.pages)):
        page = pdf_reader.pages[page_num]
        text += page.extract_text() + "\n"
    return text

def split_text_into_chunks(text, chunk_size=200, overlap=50):
    """Metni anlamlÄ± parÃ§alara bÃ¶ler."""
    # Metni cÃ¼mlelere ayÄ±r
    sentences = sent_tokenize(text)
    
    chunks = []
    current_chunk = []
    current_size = 0
    
    for sentence in sentences:
        current_chunk.append(sentence)
        current_size += len(sentence)
        
        if current_size >= chunk_size:
            # Chunk'Ä± tamamla ve listeye ekle
            chunks.append(" ".join(current_chunk))
            
            # Ã‡akÄ±ÅŸma iÃ§in son birkaÃ§ cÃ¼mleyi tut
            current_chunk = current_chunk[-3:]  # Son 3 cÃ¼mleyi tut
            current_size = sum(len(s) for s in current_chunk)
    
    # Son chunk'Ä± ekle
    if current_chunk:
        chunks.append(" ".join(current_chunk))
    
    return chunks

def semantic_search(model, query, chunks, top_k=5):
    """Anlamsal olarak en benzer metinleri bulur."""
    # Sorgu vektÃ¶rÃ¼nÃ¼ hesapla
    query_embedding = model.encode([query])[0]
    
    # TÃ¼m chunk'larÄ±n vektÃ¶rlerini hesapla
    chunk_embeddings = model.encode(chunks)
    
    # BenzerliÄŸi hesapla (kosinÃ¼s benzerliÄŸi)
    similarities = cosine_similarity([query_embedding], chunk_embeddings)[0]
    
    # En benzer top_k chunk'Ä± bul
    top_indices = similarities.argsort()[-top_k:][::-1]
    
    results = []
    for idx in top_indices:
        if similarities[idx] > 0.3:  # Minimum benzerlik eÅŸiÄŸi
            results.append({"text": chunks[idx], "score": similarities[idx]})
    
    return results

def process_pdf_with_semantic_search(pdf_file, query, top_k=5):
    """PDF'ten anlamsal arama ile ilgili bÃ¶lÃ¼mleri Ã§Ä±karÄ±r."""
    try:
        # PDF'ten metni Ã§Ä±kar
        full_text = extract_text_from_pdf(pdf_file)
        
        # Metni parÃ§alara bÃ¶l
        chunks = split_text_into_chunks(full_text)
        
        if not chunks:
            return f"PDF'den metin Ã§Ä±karÄ±lamadÄ± veya bÃ¶lÃ¼mler oluÅŸturulamadÄ±."
        
        # Model yÃ¼kleme
        model = load_model()
        
        # Anlamsal arama yap
        results = semantic_search(model, query, chunks, top_k)
        
        if not results:
            # SonuÃ§ bulunamazsa, kelime tabanlÄ± arama dene
            keyword_results = []
            for chunk in chunks:
                if query.lower() in chunk.lower():
                    keyword_results.append({"text": chunk, "score": 1.0})
            
            if keyword_results:
                results = keyword_results[:top_k]
        
        if not results:
            return f"'{query}' iÃ§in ilgili bÃ¶lÃ¼m bulunamadÄ±."
        else:
            # SonuÃ§larÄ± birleÅŸtir
            result_text = f"Arama Sorgusu: {query}\n\n"
            
            for i, res in enumerate(results, 1):
                result_text += f"--- SonuÃ§ {i} (Benzerlik: {res['score']:.2f}) ---\n\n"
                result_text += f"{res['text']}\n\n"
            
            return result_text
    
    except Exception as e:
        return f"Ä°ÅŸlem sÄ±rasÄ±nda hata oluÅŸtu: {str(e)}"

# Streamlit UygulamasÄ±
st.set_page_config(page_title="Claude DokÃ¼mantasyon AsistanÄ±", page_icon="ğŸ“š", layout="wide")

# CSS styling
st.markdown("""
<style>
    .main-header {
        color: #1E88E5;
        font-size: 2.5rem;
    }
    .sub-header {
        color: #0D47A1;
        font-size: 1.5rem;
    }
    .stTextArea textarea {
        font-family: monospace;
        font-size: 1rem;
    }
    /* Drag & Drop area styling */
    [data-testid="stFileUploader"] {
        border: 2px dashed #1E88E5;
        border-radius: 8px;
        padding: 20px;
        margin-bottom: 20px;
    }
</style>
""", unsafe_allow_html=True)

col1, col2 = st.columns([2, 1])

with col1:
    st.markdown('<h1 class="main-header">Claude DokÃ¼mantasyon AsistanÄ±</h1>', unsafe_allow_html=True)
    st.markdown("""
    PDF dokÃ¼manlarÄ±ndan anlamsal arama yaparak iÃ§erik Ã§Ä±karÄ±p Claude AI'ya aktarmanÄ±n en akÄ±llÄ± yolu.
    """)

with col2:
    st.image("https://emojipedia-us.s3.amazonaws.com/source/microsoft-teams/337/robot_1f916.png", width=100)

st.markdown('<h2 class="sub-header">PDF YÃ¼kle & Ä°Ã§erik Ara</h2>', unsafe_allow_html=True)

with st.expander("âœ¨ Yeni: Anlamsal Arama Ã–zelliÄŸi", expanded=True):
    st.markdown("""
    **ArtÄ±k arama sonuÃ§larÄ± daha akÄ±llÄ±!** Yeni anlamsal arama Ã¶zelliÄŸi sayesinde:
    
    - Tam kelime eÅŸleÅŸmesi yerine kavramsal benzerlik aranÄ±r
    - YazÄ±m hatalarÄ±na ve farklÄ± ifade biÃ§imlerine karÅŸÄ± daha dayanÄ±klÄ±
    - Daha doÄŸru ve kapsamlÄ± sonuÃ§lar elde edersiniz
    
    Bu Ã¶zellik sayesinde, aradÄ±ÄŸÄ±nÄ±z bilgiler aynÄ± kelimelerle ifade edilmemiÅŸ olsa bile bulabilirsiniz!
    """)

uploaded_file = st.file_uploader("PDF dokÃ¼manÄ±nÄ± sÃ¼rÃ¼kleyip bÄ±rakÄ±n veya seÃ§in", type="pdf", 
                                 help="Sadece PDF dosyalarÄ± desteklenmektedir.")

if uploaded_file is not None:
    pdf_name = uploaded_file.name
    st.success(f"'{pdf_name}' baÅŸarÄ±yla yÃ¼klendi! Åimdi arama sorgunuzu girin.")
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        query = st.text_input("Arama sorgusu", 
                     placeholder="Ã–rn: PPO algoritmasÄ±, DQN implementasyonu, Stable Baselines kullanÄ±mÄ±")
    
    with col2:
        top_k = st.slider("SonuÃ§ sayÄ±sÄ±", min_value=1, max_value=10, value=3, 
                         help="DÃ¶ndÃ¼rÃ¼lecek maksimum bÃ¶lÃ¼m sayÄ±sÄ±")
    
    if query:
        if st.button("Anlamsal Arama Yap", key="search_button", use_container_width=True):
            with st.spinner('Anlamsal arama yapÄ±lÄ±yor... Ä°lk Ã§alÄ±ÅŸtÄ±rmada model yÃ¼klemesi biraz zaman alabilir.'):
                result = process_pdf_with_semantic_search(uploaded_file, query, top_k)
                
                # SonuÃ§larÄ± session_state'e kaydet
                st.session_state.result = result
                st.session_state.has_result = True
    
    # EÄŸer sonuÃ§ varsa gÃ¶ster
    if 'has_result' in st.session_state and st.session_state.has_result:
        st.markdown('<h2 class="sub-header">SonuÃ§lar</h2>', unsafe_allow_html=True)
        
        # Sonucu kod bloÄŸu olarak gÃ¶ster
        st.code(st.session_state.result, language="text")
        
        # Kopyalama talimatÄ±
        st.info("ğŸ‘† YukarÄ±daki kod bloÄŸunun saÄŸ Ã¼st kÃ¶ÅŸesindeki kopyalama simgesini kullanarak tÃ¼m iÃ§eriÄŸi kopyalayabilirsiniz.")
        
        # NasÄ±l kullanÄ±lacaÄŸÄ±na dair ip uÃ§larÄ±
        with st.expander("Claude ile nasÄ±l kullanÄ±lÄ±r?", expanded=False):
            st.markdown("""
            1. Kod bloÄŸunun saÄŸ Ã¼st kÃ¶ÅŸesindeki kopyalama simgesine tÄ±klayÄ±n
            2. Claude sohbet penceresine gidin
            3. AÅŸaÄŸÄ±daki ÅŸablonu kullanabilirsiniz:
            
            ```
            AÅŸaÄŸÄ±daki dokÃ¼mantasyon iÃ§eriÄŸini incele ve bu konuda bir uzman gibi davran:
            
            [KopyaladÄ±ÄŸÄ±nÄ±z iÃ§erik buraya yapÄ±ÅŸtÄ±rÄ±lacak]
            
            Bu iÃ§eriÄŸi referans alarak sorularÄ±mÄ± yanÄ±tla ve aÃ§Ä±klamalarÄ±nda bu bilgileri kullan.
            ```
            """)

# Sidebar bilgileri
with st.sidebar:
    st.title("CAG ve Bu Uygulama HakkÄ±nda")
    
    st.markdown("""
    ## Claude Augmented Generation (CAG) Nedir?
    
    CAG, Claude'un kendi bilgi tabanÄ± dÄ±ÅŸÄ±ndaki Ã¶zel iÃ§eriklerle desteklenmesini saÄŸlayan Ã¶zelliktir. 
    Claude'un eÄŸitim verilerinde bulunmayan veya gÃ¼ncel olmayan bilgileri, dÄ±ÅŸarÄ±dan verilen 
    dokÃ¼manlara dayalÄ± olarak kullanabilmesini saÄŸlar.
    
    ## Bu UygulamanÄ±n AmacÄ±
    
    Bu uygulama, Reinforcement Learning gibi teknik konulardaki PDF dokÃ¼manlarÄ±nÄ±zdan 
    en alakalÄ± kÄ±sÄ±mlarÄ± semantik olarak Ã§Ä±kararak Claude'a aktarmanÄ±zÄ± saÄŸlar. 
    BÃ¶ylece Claude'u, kendi bilmediÄŸi spesifik dokÃ¼manlara dayalÄ± bir uzmana dÃ¶nÃ¼ÅŸtÃ¼rebilirsiniz.
    
    ## Token Penceresi Limiti
    
    Claude'un her sohbette iÅŸleyebileceÄŸi maksimum token sayÄ±sÄ± sÄ±nÄ±rlÄ±dÄ±r:
    - Claude Opus: ~200K token
    - Claude Sonnet: ~150K token
    
    Uzun PDF'lerin tamamÄ±nÄ± gÃ¶ndermek bu limiti aÅŸabilir ve hafÄ±zanÄ±n yetersiz kalmasÄ±na yol aÃ§abilir.
    Bu uygulama sayesinde sadece ilgili bÃ¶lÃ¼mleri Ã§Ä±kararak token limitini verimli kullanabilirsiniz.
    
    ## Ä°lk Prompt Olarak GÃ¶ndermenin Ã–nemi
    
    Ã‡Ä±karÄ±lan dokÃ¼mantasyonu sohbetin **ilk mesajÄ±** olarak gÃ¶ndermek kritik Ã¶neme sahiptir Ã§Ã¼nkÃ¼:
    
    1. Claude sohbetin baÅŸÄ±nda verilen bilgileri daha iyi hatÄ±rlar
    2. TÃ¼m sohbet boyunca bu bilgilere referans verebilir
    3. Konuyla alakalÄ± sorularÄ±nÄ±za dokÃ¼mana dayalÄ±, doÄŸru yanÄ±tlar alabilirsiniz
    4. Ä°lerleyen mesajlarda token limiti dolduÄŸunda bile ilk bilgileri korur
    
    BÃ¶ylece, karmaÅŸÄ±k Reinforcement Learning kavramlarÄ± hakkÄ±nda dokÃ¼manlarÄ±nÄ±za dayalÄ± tutarlÄ± ve doÄŸru yanÄ±tlar alabilirsiniz.
    """)
    
    st.markdown("---")
    
   
    
    st.title("NasÄ±l KullanÄ±lÄ±r?")
    st.markdown("""
    1. **PDF YÃ¼kle**: DokÃ¼manÄ±nÄ±zÄ± sÃ¼rÃ¼kleyip bÄ±rakÄ±n
    2. **Arama Sorgusu Girin**: Ne hakkÄ±nda bilgi aradÄ±ÄŸÄ±nÄ±zÄ± yazÄ±n
    3. **Ara**: Butona tÄ±klayarak iÃ§eriÄŸi Ã§Ä±karÄ±n
    4. **Kopyala**: Kod bloÄŸunun saÄŸ Ã¼st kÃ¶ÅŸesindeki kopyalama simgesini kullanÄ±n
    5. **Claude'a GÃ¶nder**: KopyaladÄ±ÄŸÄ±nÄ±z iÃ§eriÄŸi Claude AI'ya yapÄ±ÅŸtÄ±rÄ±n
    """)
    
    st.markdown("---")
    
    st.subheader("Anlamsal Arama HakkÄ±nda")
    st.markdown("""
    Bu uygulama, geleneksel kelime eÅŸleÅŸmesi yerine **anlamsal arama** kullanÄ±r:
    
    â€¢ AradÄ±ÄŸÄ±nÄ±z kelimelerin birebir aynÄ±sÄ± olmasa da ilgili iÃ§erikleri bulur
    â€¢ Kavramsal benzerliÄŸe dayalÄ± Ã§alÄ±ÅŸÄ±r
    â€¢ Ã–rneÄŸin "Python'da veri analizi" aramasÄ± yaptÄ±ÄŸÄ±nÄ±zda "pandas kÃ¼tÃ¼phanesi ile DataFrame iÅŸlemleri" gibi sonuÃ§lar bulabilir
    â€¢ YazÄ±m hatalarÄ±na ve farklÄ± ifade biÃ§imlerine karÅŸÄ± daha dayanÄ±klÄ±dÄ±r
    """)

# Ana sayfa aÃ§Ä±klamasÄ± (uploaded_file yoksa)
if uploaded_file is None:
    st.markdown("---")
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Ä°ÅŸleyiÅŸ")
        st.markdown("""
        1. PDF dosyanÄ±zÄ± yÃ¼kleyin (teknik dokÃ¼man, API referansÄ±, makale vb.)
        2. Aramak istediÄŸiniz konuyu veya kavramÄ± doÄŸal dilde ifade edin
        3. Uygulama PDF'ten anlamsal olarak en ilgili bÃ¶lÃ¼mleri Ã§Ä±karÄ±r
        4. Ã‡Ä±karÄ±lan iÃ§eriÄŸi Claude'a gÃ¶ndererek uzman bir asistana dÃ¶nÃ¼ÅŸtÃ¼rÃ¼n
        """)
    
    with col2:
        st.subheader("KullanÄ±m Ã–rnekleri")
        st.markdown("""
        â€¢ Teknik dokÃ¼mantasyondan "asenkron iÅŸlem yÃ¶netimi" hakkÄ±nda bÃ¶lÃ¼mleri bulma
        â€¢ Bilimsel bir makaleden "deneysel sonuÃ§lar ve bulgular" kÄ±smÄ±nÄ± Ã§Ä±karma
        â€¢ Kitaptan "konunun pratik uygulamalarÄ±" hakkÄ±ndaki aÃ§Ä±klamalarÄ± bulma
        """)

# Footer
st.markdown("---")
st.markdown("Claude DokÃ¼mantasyon AsistanÄ± | Semantik arama Ã¶zelliÄŸi ile en akÄ±llÄ± PDF iÃ§erik Ã§Ä±karma aracÄ±")